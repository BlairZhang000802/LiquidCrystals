{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import sys\n",
    "sys.path.append(\"C:\\\\Users\\\\Sam Yu\\\\Documents\\\\GitHub\\\\LiquidCrystals\")\n",
    "\n",
    "from LiquidCrystalSystem import LCSystem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets_path = \"C:\\\\Users\\\\Sam Yu\\\\Desktop\\\\School\\\\4A\\\\Phys_437A_Research_Project\\\\datasets\\\\r=14\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lc_systems_wrt_density = dict()\n",
    "\n",
    "for _path_ in os.listdir(datasets_path):\n",
    "    \n",
    "    full_path = os.path.join(datasets_path, _path_, 'instanceRun')\n",
    "    \n",
    "    if os.path.exists(os.path.join(full_path, \"MonteCarlo_Annulus_SimNotes.txt\")):\n",
    "        lc = LCSystem(lc_data_path=full_path)\n",
    "        global_packing_fraction = lc.sim_params['reduced density']\n",
    "        \n",
    "        lc_systems_wrt_density[global_packing_fraction] = lc\n",
    "        \n",
    "print(lc_systems_wrt_density.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from numpy.linalg import norm\n",
    "\n",
    "def create_feature_vectors_from_snapshot(coordinates, num_features, num_samples,\n",
    "                                         feature_func=lambda x, y: abs(np.cos(x - y))):\n",
    "    \n",
    "    assert (num_features < len(coordinates)), \\\n",
    "    f\"Number of features {num_features} cannot be greater than number of particles {len(coordinates)}\"\n",
    "    \n",
    "    #print(f\"Input coordinates: {coordinates}\")\n",
    "    \n",
    "    N = len(coordinates)\n",
    "    \n",
    "    # set the sampling rate for nearest neighbors\n",
    "    if N % num_features == 0:\n",
    "        nn_sampling_number = N / num_features - 1\n",
    "    else:\n",
    "        nn_sampling_number = math.floor(N / num_features)\n",
    "    \n",
    "    #print(f\"Nearest neighbor sampling number: {nn_sampling_number}\")\n",
    "        \n",
    "    # set a random seed for reproducibility\n",
    "    rng = np.random.default_rng(666)\n",
    "    probe_indices = rng.choice(N, size=num_samples, replace=False)\n",
    "    \n",
    "    # x, y positions for particle center of masses\n",
    "    spatial = [c[:2] for c in coordinates]\n",
    "    # angle of long axis with respect to x for particles\n",
    "    angular = [(c[-1] % np.pi) for c in coordinates]\n",
    "    \n",
    "    feature_vectors = []\n",
    "    \n",
    "    for probe_index in probe_indices:\n",
    "        \n",
    "        # list of distances relative to probe particle\n",
    "        chosen_coord = coordinates[probe_index]\n",
    "        \n",
    "        norm2 = lambda x, y: (np.array(x) - np.array(y)) @ (np.array(x) - np.array(y))\n",
    "        dist_to_P = lambda x: norm2(x[:2], chosen_coord[:2])\n",
    "        \n",
    "        nn_sorted = sorted(coordinates, key=dist_to_P)\n",
    "        #print(f\"Nearest neighbor sort: {nn_sorted}\")\n",
    "        \n",
    "        # feature vector\n",
    "        fv = []\n",
    "        \n",
    "        # add feature based on nearest neighbor distance\n",
    "        for i, c in enumerate(nn_sorted):\n",
    "            \n",
    "            if (i > 0) and (i % nn_sampling_number) == 0:\n",
    "                \n",
    "                feature = feature_func(c[-1], chosen_coord[-1])\n",
    "                fv.append(feature)\n",
    "            \n",
    "            # Break the addition of features if total number of features is met\n",
    "            if len(fv) == num_features:\n",
    "                break\n",
    "                \n",
    "        feature_vectors.append(fv)\n",
    "    \n",
    "    return feature_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_feature_vectors_from_snapshot([(1, 2, 1), (3, 4, 2), (5, 6, 3), (9, 10, 4), (5, 8, 5), (7, 8, 6), (9, 11, 7)], 2, 3,\n",
    "                                     feature_func=(lambda x, y: x-y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_lc_system = lc_systems_wrt_density[0.2913752913752914]\n",
    "\n",
    "print(test_lc_system.sim_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_coords = test_lc_system.system_state_at_step[19999]\n",
    "feature_vecs = create_feature_vectors_from_snapshot(test_coords, num_features=10, num_samples=5)\n",
    "\n",
    "print(feature_vecs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_features = 10\n",
    "datapoints_per_step = 5\n",
    "\n",
    "# number of rows AKA number of data-points\n",
    "# = number of densities * number of captures per density >= 1e6 * number of data-points per capture \n",
    " \n",
    "number_of_densities = len(lc_systems_wrt_density)\n",
    "number_of_captures = len([step for step in\n",
    "                          lc_systems_wrt_density[0.2913752913752914].system_state_at_step\n",
    "                          if step >= 1e6])\n",
    "\n",
    "n_rows = number_of_densities * number_of_captures * datapoints_per_step\n",
    "n_cols = number_of_features\n",
    "\n",
    "data_matrix = []\n",
    "# iterate over densities\n",
    "for density in lc_systems_wrt_density.keys():\n",
    "    \n",
    "    system_state_over_steps = lc_systems_wrt_density[density].system_state_at_step\n",
    "    \n",
    "    # iterate over Monte Carlo steps\n",
    "    for mc_step in system_state_over_steps:\n",
    "        \n",
    "        if mc_step >= 1e6:\n",
    "            \n",
    "            snapshot = system_state_over_steps[mc_step]\n",
    "            \n",
    "            feature_vecs = create_feature_vectors_from_snapshot(snapshot, number_of_features, datapoints_per_step)\n",
    "            data_matrix = data_matrix + feature_vecs\n",
    "\n",
    "data_matrix = np.stack(data_matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA()\n",
    "pca.fit(data_matrix)\n",
    "\n",
    "print(pca.components_)\n",
    "print(pca.explained_variance_ratio_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
